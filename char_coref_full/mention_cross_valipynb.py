# -*- coding: utf-8 -*-
"""mention_cross_valipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLhV-2PvnogTxL0Por_v-KN97Vf5vuOf
"""

from google.colab import drive
import itertools
drive.mount('/content/drive')

!pip install transformers
!pip install -r requirements.txt
!git clone https://github.com/conll/reference-coreference-scorers
!git clone https://github.com/dbamman/lrec2020-coref

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP_CW/LH/long-doc-coref_edit/src

default_params = {'base_data_dir': '../data', 'dataset': 'litbank_person_only', 'base_model_dir': '../models', 'model_size': 'large', 'doc_enc': 'overlap',
                 'pretrained_bert_dir': '../resources', 'max_segment_len': 512, 'top_span_ratio': 0.3, 'ment_emb': 'attn', 'max_span_width': 20,
                 'mlp_depth': 1, 'mlp_size': 3000, 'cross_val_split': 0, 'batch_size': 1, 'num_train_docs': None,
                 'dropout_rate': 0.3, 'max_epochs': 15, 'seed': 0, 'init_lr': 0.0005, 'checkpoint': False, 'eval': False, 'slurm_id': None, 'eval_model': 'paper_model', 'crossval': False}
hyper_params = {'top_span_ratio': [.2,.225, .25, .275, .3, .325], 'mlp_size': [2500, 3000, 3500] }

arguments = default_params
def cross_val(hyper_params):
  best_prec = 0
  vals = list(hyper_params.values())
  params = list(hyper_params.keys())
  pairs = itertools.product(vals[0], vals[1]) 
  for pair in pairs:
    arguments = default_params
    arguments[params[0]] = pair[0]
    arguments[params[1]] = pair[1]
    model = main_class(arguments)
    res = model.run()
    prec = res['test']['precision']
    if prec > best_prec:
      best_prec = prec
      print('\n \n New Best Params:\n', arguments, '\n \n ')
      best_arguments = arguments
      best_res = res
  return best_arguments, best_res['dev'], best_res['test']

best_args, best_dev, best_res = cross_val(hyper_params)

import argparse
import os
from os import path
import logging
import sys
import pdb
sys.path.insert(1, '/content/drive/MyDrive/NLP_CW/LH/long-doc-coref_edit/src')
from mention_model.experiment import Experiment
from mention_model.utils import get_mention_model_name


logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)

class main_class:
  def __init__(self, new_args):
    self.args = {'base_data_dir': '../data', 'dataset': 'litbank_person_only', 'base_model_dir': '../models', 'model_size': 'large', 'doc_enc': 'overlap',
                 'pretrained_bert_dir': '../resources', 'max_segment_len': 512, 'top_span_ratio': 0.3, 'ment_emb': 'attn', 'max_span_width': 20,
                 'mlp_depth': 1, 'mlp_size': 3000, 'cross_val_split': 0, 'batch_size': 1, 'num_train_docs': None,
                 'dropout_rate': 0.3, 'max_epochs': 15, 'seed': 0, 'init_lr': 0.0005, 'checkpoint': False, 'eval': False, 'slurm_id': None, 'eval_model': 'paper_model', 'crossval': False}
  
    for new_arg in new_args.keys():
       self.args[new_arg] = new_args[new_arg]
    model_name = get_mention_model_name(self.args)
    print(model_name)
    if not self.args['eval']:
      self.args['model_dir'] = path.join(self.args['base_model_dir'], model_name)
      self.args['best_model_dir'] = path.join(self.args['model_dir'], 'best_models')
      if not path.exists(self.args['model_dir']):
          os.makedirs(self.args['model_dir'] )
      if not path.exists(self.args['best_model_dir']):
          os.makedirs(self.args['best_model_dir'])
    else:
      self.args['best_model_dir'] = path.join(self.args['base_model_dir'], self.args['eval_model'],'best_models')
      self.args['model_dir']= self.args['best_model_dir']
    if (self.args['dataset'] == 'litbank') | (self.args['dataset'] == 'litbank_person_only'):
        self.args['data_dir'] = path.join(self.args['base_data_dir'], f'{self.args["dataset"]}/{self.args["doc_enc"]}/{self.args["cross_val_split"]}')

    
  def run(self):
    if self.args['crossval']== True:
      res = Experiment(self.args, **self.args)
      return res.final_eval()
    else:
      Experiment(**self.args)