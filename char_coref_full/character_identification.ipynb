{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_identification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDNvgVTzsHnh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/jblack97/long-doc-coref.git\n",
        "!pip install  allennlp==2.4.0 allennlp-models==2.4.0\n",
        "!pip install spacy==3.2\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('long-doc-coref/src')\n",
        "sys.path.append('NLP_CW')\n",
        "sys.path.append('character_relationship_analysis/scripts')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "from google.colab import drive\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "import tqdm\n",
        "import json\n",
        "import utils\n",
        "import pickle\n",
        "drive.mount('/content/gdrive')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# This will also download the SpanBERT model finetuned for Coreference (by Joshi et al, 2020) from Huggingface\n",
        "from inference.inference import Inference\n",
        "from  inference.tokenize_doc import *\n",
        "from transformers import BertTokenizerFast"
      ],
      "metadata": {
        "id": "J6iWB9FQsNee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of files\n",
        "file_list = ['peter_pan.txt', 'winnie_the_pooh.txt', 'harry_potter_1.txt', 'dracula.txt',\n",
        "             'charlie_and_the_chocolate_factory.txt']"
      ],
      "metadata": {
        "id": "N6e5ciN7sR_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapping from file name to book title\n",
        "file_to_title = {'dracula.txt':'Dracula', 'charlie_and_the_chocolate_factory.txt': 'Charlie and the Chocolate Factory', 'winnie_the_pooh.txt':'Winnie the Pooh', 'peter_pan.txt':'Peter Pan',\n",
        "                 'harry_potter_1.txt':'Harry Potter Book 1'}"
      ],
      "metadata": {
        "id": "WsqQoi0QsVqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "lG-0aXdWsVl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading spacy model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp.max_length = 5000000"
      ],
      "metadata": {
        "id": "rrVkAucRsdn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of books\n",
        "books = []\n",
        "\n",
        "#List of characters for each book\n",
        "char_lists = []\n",
        "\n",
        "for book in tqdm.tqdm(file_list):\n",
        "\n",
        "  books.append(book)\n",
        "\n",
        "  #Read book from text file\n",
        "  with open(f'character_relationship_analysis/data/texts/{book}') as f:\n",
        "    doc = f.read()\n",
        "  doc = re.sub('\\n', ' ', doc)\n",
        "\n",
        "  #Named entity extraction\n",
        "  ner = nlp(doc)\n",
        "  entity = []\n",
        "  start_idx = []\n",
        "  end_idx = []\n",
        "  ent_type = []\n",
        "\n",
        "  for ent in ner.ents:\n",
        "    entity.append(ent.text)\n",
        "    ent_type.append(ent.label_)\n",
        "    start_idx.append(ent.start_char)\n",
        "    end_idx.append(ent.end_char)\n",
        "\n",
        "  #Character dataframe\n",
        "  ner_df = pd.DataFrame(list(zip(entity, ent_type, start_idx, end_idx )),\n",
        "                      columns=['entity', 'entity_type', 'start_idx', 'end_idx'])\n",
        "  \n",
        "  #Capitalizing entity names\n",
        "  ner_df['entity'] = ner_df['entity'].apply(lambda x: x.title())\n",
        "\n",
        "  #Top 20 most frequent characters mentioned\n",
        "  char_list = list(ner_df[ner_df['entity_type'] == 'PERSON']['entity'].value_counts()[:20].index)\n",
        "\n",
        "  char_lists.append(char_list)"
      ],
      "metadata": {
        "id": "U-pV-Fpssdf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataframe of all books and their top 20 characters\n",
        "char_df = pd.DataFrame(columns=['book', 'character'])\n",
        "char_book = []\n",
        "char_name = []\n",
        "\n",
        "for i, book in enumerate(books):\n",
        "  for j, char in enumerate(char_lists[i]):\n",
        "    char_book.append(book)\n",
        "    char_name.append(char)\n",
        "\n",
        "char_df['book'] = char_book\n",
        "char_df['character'] = char_name"
      ],
      "metadata": {
        "id": "oqoOFrkCsdNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to process character name tuple and resolve issues such\n",
        "as possessives being attached to the end of a character name.\n",
        "Returns string of processed character name.\n",
        "'''\n",
        "def process_name_tuple(name_tuple):\n",
        "  if len(name_tuple) > 1:\n",
        "    name_str = ''\n",
        "    for i, c in enumerate(name_tuple):\n",
        "      if i <  (len(name_tuple) - 1):\n",
        "        #Resolve possessives\n",
        "        if ((name_tuple[i] == \"â€™\") | ((name_tuple[i] == \"'\"))) & (name_tuple[i+1]=='S'):\n",
        "          name_str += ''\n",
        "          break\n",
        "        #Ensure space not added before '-' or '.' \n",
        "        elif (name_tuple[i+1] == '-') | (name_tuple[i+1] == '.'):\n",
        "          name_str += c\n",
        "        #Ensure space not added after '-'\n",
        "        elif (name_tuple[i] == '-'):\n",
        "          name_str += c\n",
        "        else:\n",
        "          name_str += c + ' '\n",
        "      else:\n",
        "        name_str += c\n",
        "  else:\n",
        "    name_str = name_tuple[0]\n",
        "\n",
        "  return name_str.strip()"
      ],
      "metadata": {
        "id": "erSV2rw1snc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Given a dataframe of NER output, function reduces some of the issues with NER output, such \n",
        "as having multiple entities for one character.\n",
        "'''\n",
        "def process_chars(char_df, books):\n",
        "  titles = [\"Dr\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\", \"Miss\", \"Mrs.\", \"Mrs\", \"Monsieur\", \"Madame\"]\n",
        "  char_df['name_tuple'] =  char_df['character'].apply(lambda x: tuple(utils.inv_map(flatten(get_tokenized_doc(x, tokenizer)['sentences']))))\n",
        "  new_char_list = []\n",
        "  new_book_list = []\n",
        "\n",
        "  for book in books:\n",
        "    book_chars = []\n",
        "    chars = list(char_df[char_df.book == book]['name_tuple'])\n",
        "\n",
        "    for i, char in enumerate(chars):\n",
        "        \n",
        "        if len(char) == 1:\n",
        "          '''\n",
        "          Replacing forename-surname with forename, if forenames match and forename\n",
        "          is include in character list (e.g. 'Charles Darnay' -> 'Charles')\n",
        "          '''\n",
        "          for j, ref_char in enumerate(chars):\n",
        "            if i != j:\n",
        "              if (char[0] == ref_char[0]) & (char[0] not in titles):\n",
        "                chars[j] = (char[0], '')\n",
        "\n",
        "        #Removing instances of single letter characters (e.g. K. and I.)\n",
        "        if len(char) == 2:\n",
        "          if (len(char[0]) == 1) & (char[1] == '.'):\n",
        "            chars.remove(char)\n",
        "\n",
        "      \n",
        "    new_chars = [process_name_tuple(x) for x in chars]\n",
        "    \n",
        "    unique_chars = set(new_chars)\n",
        "    unique_chars = list(unique_chars) \n",
        "    new_char_list.extend(unique_chars)\n",
        "    new_book_list.extend([book] * len(unique_chars))\n",
        "\n",
        "  new_char_df = pd.DataFrame(list(zip(new_book_list, new_char_list)),\n",
        "                      columns=['file', 'character'])\n",
        "  new_char_df['book'] = new_char_df['file'].apply(lambda x: file_to_title[x])\n",
        "  return new_char_df"
      ],
      "metadata": {
        "id": "qJtdf1ltsqvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing NER output\n",
        "unique_books = set(char_book)\n",
        "new_char_df = process_chars(char_df, unique_books)"
      ],
      "metadata": {
        "id": "134tKBmCsrnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counting number of sentences with at least two characters\n",
        "#NOTE: as per the literature, sentences with > 3 characters \n",
        "#are not included as interactions between each of the \n",
        "#mentioned characters\n",
        "\n",
        "files = []\n",
        "shared_sents = []\n",
        "for book in tqdm.tqdm(file_list):\n",
        "\n",
        "  #Read book from text file\n",
        "  with open(f'character_relationship_analysis/data/texts/{book}') as f:\n",
        "    doc = f.read()\n",
        "  doc = re.sub('\\n', ' ', doc)\n",
        "\n",
        "  chars = list(new_char_df[new_char_df['file']==book]['character'])\n",
        "\n",
        "  num_shared = 0\n",
        "\n",
        "  for sent in sent_tokenize(doc):\n",
        "    num_chars = sum([int(char in sent) for char in chars])\n",
        "    if num_chars > 1:\n",
        "      if num_chars == 2:\n",
        "        num_shared += 1\n",
        "      elif num_chars == 3:\n",
        "        num_shared += 3\n",
        "      else:\n",
        "        num_shared += 1\n",
        "\n",
        "  files.append(book)\n",
        "  shared_sents.append(num_shared)\n",
        "\n",
        "shared_sent_df = pd.DataFrame(list(zip(files, shared_sents)),\n",
        "                      columns=['file', 'shared_sentences'])\n",
        "shared_sent_df['book'] = shared_sent_df['file'].apply(lambda x: file_to_title[x])"
      ],
      "metadata": {
        "id": "21FY6lYY1WcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shared_sent_df.to_csv('character_relationship_analysis/data/shared_sentences_no_coref.csv', index=False)\n",
        "new_char_df.to_csv('character_relationship_analysis/data/book_characters.csv', index=False)"
      ],
      "metadata": {
        "id": "RivxPgxg8HaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
