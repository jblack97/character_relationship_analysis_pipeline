{
  "cells": [
   
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrSQHlez__NH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('long-doc-coref/src')\n",
        "sys.path.append('NLP_CW')\n",
        "sys.path.append('character_relationship_analysis/data')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "import nltk\n",
        "import re\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "import tqdm\n",
        "import json\n",
        "import utils\n",
        "import pickle\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# This will also download the SpanBERT model finetuned for Coreference (by Joshi et al, 2020) from Huggingface\n",
        "from inference.inference import Inference\n",
        "from  inference.tokenize_doc import *\n",
        "from transformers import BertTokenizerFast\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNJ8mtMM__I0"
      },
      "outputs": [],
      "source": [
        "#Bert tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05lXFayAAJkS"
      },
      "outputs": [],
      "source": [
        "#Semantic role labelling model\n",
        "srl_model = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG-UfI0iARG-"
      },
      "outputs": [],
      "source": [
        "#Sentiment Analysis Model\n",
        "sentiment = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/stanford-sentiment-treebank-roberta.2021-03-11.tar.gz\")\n",
        "sent_analyser = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_PlGqVzbY7p"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function returns sentiment score between -1 (negative) and +1 (positive) \n",
        "for a given string.\n",
        "'''\n",
        "def get_sentiment(string):\n",
        "  output = sentiment.predict(string)\n",
        "  sent_score = (output['probs'][0]-0.5)*2\n",
        "\n",
        "  return sent_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnPvLMxAea8"
      },
      "outputs": [],
      "source": [
        "#Semantic role labelling\n",
        "def perform_srl(coref_output, char_pairs, pair_ids):\n",
        "  #Dataframe to store SRL output\n",
        "  srl_df = pd.DataFrame(columns = ['pair_id','char_list', 'srl_output'])\n",
        "\n",
        "  #List of character lists, IDs\n",
        "  char_lists = []\n",
        "  char_pair_ids = []\n",
        "\n",
        "  #List to store the index of where first word of sentence occurs in overall document\n",
        "  sent_ids = []\n",
        "\n",
        "  #List of SRL outputs\n",
        "  srl_outputs = []\n",
        "\n",
        "  for id in pair_ids:\n",
        "    char_list = char_pairs[id]\n",
        "    num_shared_sents = len(coref_doc[id])\n",
        "    \n",
        "    #Iterate through shared sentences\n",
        "    for sent_id in range(num_shared_sents):\n",
        "      char_lists.append(char_list)\n",
        "      char_pair_ids.append(id)\n",
        "      shared_sent = coref_doc[id][sent_id][1]\n",
        "      sent_idx = coref_doc[id][sent_id][0]\n",
        "      sent_ids.append(sent_idx)\n",
        "      sent_tok = nltk.sent_tokenize(shared_sent)\n",
        "      srl = srl_model.predict(sent_tok[0])\n",
        "      srl_outputs.append(srl)\n",
        "\n",
        "  srl_df['pair_id'] = char_pair_ids\n",
        "  srl_df['char_list'] = char_lists\n",
        "  srl_df['sent_id'] = sent_ids\n",
        "  srl_df['srl_output'] = srl_outputs\n",
        "\n",
        "  return srl_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQlyaFuuAlt6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Given a segment of output from Semantic Role Labelling, determines if the\n",
        "ARG0, verb, and ARG1 arguments are all present in the output.\n",
        "'''\n",
        "def args_present_1(tag_list):\n",
        "\n",
        "  #Sequence tags to search for\n",
        "  arg_0 = 'B-ARG0'\n",
        "  arg_1 = 'B-ARG1'\n",
        "  verb = 'B-V'\n",
        "\n",
        "  if (arg_0 in tag_list) & (arg_1 in tag_list) & (verb in tag_list):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQYvjEc1AnNq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Given a segment of output from Semantic Role Labelling, determines if the\n",
        "ARG0, verb, and ARG2 arguments are all present in the output.\n",
        "'''\n",
        "def args_present_2(tag_list):\n",
        "\n",
        "  #Sequence tags to search for\n",
        "  arg_0 = 'B-ARG0'\n",
        "  arg_2 = 'B-ARG2'\n",
        "  verb = 'B-V'\n",
        "\n",
        "  if (arg_0 in tag_list) & (arg_2 in tag_list) & (verb in tag_list):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "457LbyjPAqg6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Given a segment of output from Semantic Role Labelling and the original tokenized sentence,\n",
        "determines whether there are characters present in both of the object and subject parts of text.\n",
        "If so, returns a dataframe containing, the subject, object, verb, sentence, sub-sentence and the sentiment scores\n",
        "for the sentence, sub-sentence, and verb.\n",
        "'''\n",
        "def extract_sentiment(text, char_list, word_list, arg_1_present, sent_loc):\n",
        "\n",
        "  #Dataframe to store character interaction outputs\n",
        "  char_int_df = pd.DataFrame(columns=['sent_loc', 'subject', 'object', 'sub_sentence', 'verb', 'sentence', 'sub_sent_score', 'verb_score', 'sent_score'])\n",
        "  \n",
        "  #Extracting arguments\n",
        "  arg_0 = re.search('ARG0: (.+?)]', text).group(1)\n",
        "\n",
        "  #Determining whether to search for object in ARG1 or ARG2\n",
        "  if arg_1_present:\n",
        "    arg_1 = re.search('ARG1: (.+?)]', text).group(1)\n",
        "  else:\n",
        "    arg_1 = re.search('ARG2: (.+?)]', text).group(1)\n",
        "\n",
        "  verb = re.search('V: (.+?)]', text).group(1)\n",
        "\n",
        "  arg_0_search = str(arg_0.lower())\n",
        "  arg_1_search = str(arg_1.lower())\n",
        "\n",
        "  #Finding sub-sentence capturing subject, object, and verb\n",
        "  arg_0_tok = \" \".join(get_tokenized_doc(arg_0, tokenizer)['sentences'][0])\n",
        "  arg_1_tok = \" \".join(get_tokenized_doc(arg_1, tokenizer)['sentences'][0])\n",
        "  verb_tok = \" \".join(get_tokenized_doc(verb, tokenizer)['sentences'][0])\n",
        "  words = \" \".join(word_list)\n",
        "  words_lower_list = [x.lower() for x in word_list]\n",
        "  words_lower = \" \".join(words_lower_list)\n",
        "\n",
        "  #Lists to store values\n",
        "  subjects = []\n",
        "  objects = []\n",
        "  sub_sents = []\n",
        "  sents = []\n",
        "  verbs = []\n",
        "  verb_scores = []\n",
        "  sub_sent_scores = []\n",
        "  sent_scores = []\n",
        "  sent_locs = []\n",
        "\n",
        "  #Only extract if both subject and object are characters\n",
        "  for char_1 in char_list:\n",
        "    for char_2 in char_list:\n",
        "      \n",
        "      char_1_search = str(char_1.lower())\n",
        "      char_2_search = str(char_2.lower())\n",
        "\n",
        "      if (char_1_search in arg_0_search) & (char_2_search in arg_1_search) & (char_1_search != char_2_search):\n",
        "\n",
        "        #Extracting sub-sentence based on the order of which the arguments occur\n",
        "        arg_0_id = words_lower.index(arg_0.lower())\n",
        "        arg_1_id = words_lower.index(arg_1.lower())\n",
        "        verb_tok_id = words.index(verb)\n",
        "\n",
        "        if arg_0_id != arg_1_id:\n",
        "\n",
        "          order_dict = {arg_0: arg_0_id, arg_1: arg_1_id, verb: verb_tok_id}\n",
        "\n",
        "          start = min(order_dict, key=order_dict.get)\n",
        "          end = max(order_dict, key=order_dict.get)\n",
        "          \n",
        "          search_str = r'' + start + '.+?' + end + ''\n",
        "          \n",
        "          #Replacing brackets with special characters that break regex function\n",
        "          search_str = search_str.replace('(', '-')\n",
        "          search_str = search_str.replace(')', '-')\n",
        "          search_str = search_str.replace('*', ' ')\n",
        "          search_str = search_str.replace('/', ' ')\n",
        "          search_str = search_str.replace('\\\\', ' ')\n",
        "          words = words.replace('(', '-')\n",
        "          words = words.replace(')', '-')\n",
        "          words = words.replace('*', ' ')\n",
        "          words = words.replace('/', ' ')\n",
        "          words = words.replace('\\\\', ' ')\n",
        "            \n",
        "          sub_sent = re.findall(search_str, words, re.IGNORECASE)[0]\n",
        "          sub_sents.append(sub_sent)\n",
        "          sents.append(words)\n",
        "\n",
        "          subjects.append(char_1)\n",
        "          objects.append(char_2)\n",
        "          verbs.append(verb)\n",
        "\n",
        "          #Sentiment Analysis on verb and sub-sentence\n",
        "          sub_sent_score = get_sentiment(sub_sent)\n",
        "          verb_score = get_sentiment(verb)\n",
        "          sent_score = get_sentiment(words)\n",
        "          sub_sent_scores.append(sub_sent_score)\n",
        "          sent_scores.append(sent_score)\n",
        "          verb_scores.append(verb_score)\n",
        "          sent_locs.append(sent_loc)\n",
        "\n",
        "  char_int_df['sent_loc'] = sent_locs\n",
        "  char_int_df['subject'] = subjects\n",
        "  char_int_df['object'] = objects\n",
        "  char_int_df['sub_sentence'] = sub_sents\n",
        "  char_int_df['verb'] = verbs\n",
        "  char_int_df['sentence'] = sents\n",
        "  char_int_df['sub_sent_score'] = sub_sent_scores\n",
        "  char_int_df['verb_score'] = verb_scores\n",
        "  char_int_df['sent_score'] = sent_scores\n",
        "     \n",
        "  return char_int_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiaIyvbqAtEL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Given a list of characters and dataframe of SRL outputs, returns\n",
        "a dataframe of all relevant character interactions and sentiment scores\n",
        "for each interaction.\n",
        "'''\n",
        "def process_srl(char_pair_list, char_pair_ids, srl_df):\n",
        "\n",
        "  #Character interaction dataframe\n",
        "  interaction_df = pd.DataFrame(columns=['subject', 'object', 'sub_sentence', 'verb', 'sentence', 'sub_sent_score', 'verb_score', 'sent_score'])\n",
        "\n",
        "  for char_pair_id in char_pair_ids:\n",
        "    \n",
        "    char_srl_output = list(srl_df[srl_df['pair_id'] == char_pair_id]['srl_output'])\n",
        "    sent_locs = list(srl_df[srl_df['pair_id'] == char_pair_id]['sent_id'])\n",
        "    char_list = list(srl_df[srl_df['pair_id'] == char_pair_id]['char_list'])\n",
        "\n",
        "    if len(char_list) > 0:\n",
        "      char_pair = char_list[0]\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "    #Iterate through SRL output for each character pair\n",
        "    for i, srl_output in enumerate(char_srl_output):\n",
        "      sent_loc = sent_locs[i]\n",
        "      words = srl_output['words']\n",
        "      seq = srl_output['verbs']\n",
        "      num_seq = len(seq)\n",
        "\n",
        "      if num_seq > 0:\n",
        "        for j, sent in enumerate(seq):\n",
        "          tags = sent['tags']\n",
        "          text = sent['description']\n",
        "\n",
        "          #Check that ARG0 and ARG1 are present\n",
        "          if args_present_1(tags):\n",
        "            char_int_df = extract_sentiment(text, char_pair, words, True, sent_loc)\n",
        "            interaction_df = pd.concat([interaction_df, char_int_df])\n",
        "\n",
        "          #If ARG1 not present, check for ARG0 and ARG2\n",
        "          elif args_present_2(tags):\n",
        "            char_int_df = extract_sentiment(text, char_pair, words, False, sent_loc)\n",
        "            interaction_df = pd.concat([interaction_df, char_int_df])\n",
        "\n",
        "  return interaction_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-XR92p7Av7W"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Removes common uncode characters from text\n",
        "'''\n",
        "def remove_unicode(data):\n",
        "  data = data.replace('\\\\u201c', '')\n",
        "  data = data.replace('\\\\u201d', '')\n",
        "  data = data.replace('\\\\u2019', '')\n",
        "  data = data.replace('\\\\u2014', '')\n",
        "  data = data.replace('\\\\u00', '')\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vlskoP9Az_X"
      },
      "outputs": [],
      "source": [
        "book_list = ['chocolate_factory', 'dracula', 'harry potter book 1', 'peter_pan', 'winnie_the_pooh'] \n",
        "\n",
        "#Choose model type\n",
        "model = 'new_model'\n",
        "\n",
        "#Dataframe to store character relationship data for all books\n",
        "char_sent_df = pd.DataFrame(columns=['book', 'subject', 'object', 'sub_sentence', 'verb', 'sentence', 'sub_sent_score', 'verb_score', 'sent_score'])\n",
        "\n",
        "for book in tqdm.tqdm(book_list):\n",
        "  #Read character sentence pairs\n",
        "  with open(f'character_relationship_analysis/data/final/shared sentences/{model}/{book}/pair_replace.json') as f:\n",
        "    coref_doc = json.load(f)\n",
        "  coref_doc = re.sub('\\n', ' ', coref_doc)\n",
        "\n",
        "  #Remove common unicode characters\n",
        "  coref_doc = remove_unicode(coref_doc)\n",
        "\n",
        "  #Reading as dict\n",
        "  coref_doc = json.loads(coref_doc)\n",
        "\n",
        "  #Read character paid IDs\n",
        "  with open(f'character_relationship_analysis/data/final/shared sentences/{model}/{book}/encoding.json') as f:\n",
        "    pair_id = json.load(f)\n",
        "  pair_id = re.sub('\\n', ' ', pair_id)\n",
        "\n",
        "  #Remove common unicode characters\n",
        "  pair_id = remove_unicode(pair_id)\n",
        "\n",
        "  #Reading as dict\n",
        "  char_pair_dict = json.loads(pair_id)\n",
        "\n",
        "  #Relationship Pair IDs\n",
        "  pair_ids = coref_doc.keys()\n",
        "\n",
        "  #SRL Dataframe\n",
        "  srl_df = perform_srl(coref_doc, char_pair_dict, pair_ids)\n",
        "\n",
        "  #List of all character pairs\n",
        "  char_pairs = np.unique(srl_df['char_list']).tolist()\n",
        "\n",
        "  #Interaction dataframe\n",
        "  int_df = process_srl(char_pairs, pair_ids, srl_df)\n",
        "\n",
        "  #Remove duplicate rows\n",
        "  int_df.drop_duplicates(inplace=True)\n",
        "\n",
        "  #Interaction Dataframe\n",
        "\n",
        "  int_df['book'] = book\n",
        "\n",
        "  char_sent_df = pd.concat([char_sent_df, int_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbTC_W8gA7eR"
      },
      "outputs": [],
      "source": [
        "#Vader sentiment scores\n",
        "char_sent_df['sub_sent_vader'] = char_sent_df['sub_sentence'].apply(lambda x: sent_analyser.polarity_scores(x)['compound'])\n",
        "char_sent_df['sent_vader'] = char_sent_df['sentence'].apply(lambda x: sent_analyser.polarity_scores(x)['compound'])\n",
        "char_sent_df['verb_vader'] = char_sent_df['verb'].apply(lambda x: sent_analyser.polarity_scores(x)['compound'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ0f58ThBuN-"
      },
      "outputs": [],
      "source": [
        "#Sets of character pairs\n",
        "char_sent_df['pair'] = char_sent_df.apply(lambda x: sorted([x['subject'], x['object']]),axis=1)\n",
        "char_sent_df['pair'] = char_sent_df['pair'].apply(lambda x: set(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MX9gc-rBYtI"
      },
      "outputs": [],
      "source": [
        "#Map file name to book title\n",
        "title_dict = {'dracula':'Dracula', 'chocolate_factory': 'Charlie and the Chocolate Factory', 'winnie_the_pooh':'Winnie the Pooh', 'peter_pan':'Peter Pan',\n",
        "              'harry potter book 1':'Harry Potter Book 1'}\n",
        "char_sent_df['title'] = char_sent_df['book'].apply(lambda x: title_dict[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ3FUeqDB_tS"
      },
      "outputs": [],
      "source": [
        "#Remove duplicates and sort\n",
        "char_sent_df = char_sent_df.drop_duplicates(subset=['sub_sentence', 'verb', 'subject', 'object'])\n",
        "char_sent_df.sort_values(by=['title', 'subject', 'object', 'sent_loc'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g6ZRBcbCEnz"
      },
      "outputs": [],
      "source": [
        "char_sent_df.to_csv(f'character_relationship_analysis/data/{model}_all_sentiment_results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "extract_interactions_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
